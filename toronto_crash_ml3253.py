# -*- coding: utf-8 -*-
"""toronto_crash_ML3253.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oiVD4FMFtVpF36yPj4iRYG2z6ws8fvye

# Toronto Traffic Collisions: Machine Learning

This notebook will be used for the ML 3253 course for U of T. We will use the Toronto Police traffic collision dataset of seriously injured or killed (KSI) accidents to try to predict when accidents involve a fatality.
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing libraries and setting up paramters
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt 
import seaborn as sns
# %matplotlib inline

np.random.seed(404)

"""# Importing data

The CSV files to github to easily import without having to upload files into colab
"""

# Importing data
ksi = pd.read_csv("https://raw.githubusercontent.com/seanharrigan/ml_toronto_ksi/main/KSI%20(Original_%20Raw).csv")
wthr = pd.read_csv("https://raw.githubusercontent.com/seanharrigan/ml_toronto_ksi/main/Toronto_temp%20(original_raw)%20(1).csv")
holidays = pd.read_csv("https://raw.githubusercontent.com/uWaterloo/Datasets/master/Holidays/holidays.csv")

# View data
ksi.head()

# Data shape
ksi.shape

# Missing values
ksi.isna().sum().sort_values()

# KSI info
ksi.info()

ksi['DATE']

wthr.info()

holidays.info()

holidays.head(5)

"""# Data preperation and pre-processing

# Cleaning data
"""

# Renaming data
df = ksi

# Setting all column names to lower case
df.columns= df.columns.str.lower()

# Fixing date variable
df['date'] = pd.to_datetime(df.date, utc = True)

# Fill the NaN with 0 then cast to int via https://stackoverflow.com/a/43835056
df['wardnum'] = pd.to_numeric(df['wardnum'], errors='coerce').fillna(0).astype(int)

display(df['wardnum'].unique())
display(df['wardnum'].unique().size)

# Wardnum is now a int with 0 filled in (0 means nothing)
# For location the neighbourhood number (hood_140 or hood_158) might be better
# https://www.toronto.ca/city-government/data-research-maps/neighbourhoods-communities/neighbourhood-profiles/about-toronto-neighbourhoods/
# hood_140 is the correct numbering for our data time span. hood_158 has a little less missing data ('NSA')

hood140 = df[df['hood_140'] == 'NSA']
print("\nData rows with hood_140 being NSA: " + str(hood140['hood_140'].size))

hood158 = df[df['hood_158'] == 'NSA']
print("\nData rows with hood_158 being NSA: " + str(hood158['hood_158'].size))

# Data preperations (deriving variables, dropping columns, sorting, etc.)
df = (df
      # Dropping un-needed columns
      .drop(columns=['index_', 'objectid', 'cyccond', 'cycact', 'cyclistype', 'x', 'y'])
      # Sorty by date and accnum (this variable is crash ID)
      .sort_values(['date', 'accnum'], ascending = [True, True])
)

# Creating dervived variables
df = (df           
      .assign(
           # Creating variables
           month = df.date.dt.month, 
           day = df.date.dt.day, 
           wkday = df.date.dt.weekday,
           wkday_cat = df.date.dt.day_name(),
           # Outcome variable
           y_ksi = np.where(((df['injury'] == 'Major') | (df['injury'] == 'Fatal')), 1, 0),
           y_dth = np.where((df['injury'] == 'Fatal'), 1, 0),
           # Fixing missing values when appropriate
           # accloc = df.accloc.fillna('Not at intersection'), # unsure about this variable
           motorcycle = df.motorcycle.fillna('Not at intersection'),
           truck = df.truck.fillna('No'),
           trsn_city_veh = df.trsn_city_veh.fillna('No'),
           emerg_veh = df.emerg_veh.fillna('No'),
           passenger = df.passenger.fillna('No'),
           speeding = df.speeding.fillna('No'),
           ag_driv = df.ag_driv.fillna('No'),
           redlight = df.redlight.fillna('No'),
           alcohol = df.alcohol.fillna('No'),
           disability = df.disability.fillna('No'),
           automobile = df.automobile.fillna('No'),
           pedestrian = df.pedestrian.fillna('No'),
           cyclist = df.cyclist.fillna('No')
      )               
)


df.head()

df.dtypes

df.columns

# Removing rows with holiday == 'Additional Day' from holidays DataFrame
# businesses = businesses.loc[businesses['categories'] == 'Restaurants']
additionalDays = holidays.loc[holidays['holiday'] == "Additional Day"]
holidays = holidays.loc[holidays['holiday'] != "Additional Day"]

additionalDays.info()

# Fixing date variable
holidays['date'] = pd.to_datetime(holidays.date, utc = True)

holidays.info()

# add public_holiday true/false if date is in holidays DateFrame
# https://stackoverflow.com/questions/50449088/check-if-value-from-one-dataframe-exists-in-another-dataframe
df['public_holiday'] = df['date'].dt.date.isin(holidays['date'].dt.date)

df['public_holiday'].value_counts()

"""## Checking distributions"""

# Plot a pie chart for the target to see if data are balanced
def check_data_balance(series, style="seaborn-pastel"):
  with plt.style.context(style):
    unique = series.value_counts()
    display(unique) #show unique value counts of the target
    plt.pie(unique, explode=[0.05]*len(unique), labels=unique.index, autopct='%1.1f%%');

check_data_balance(df["y_ksi"])

check_data_balance(df["y_dth"])

# Commented out IPython magic to ensure Python compatibility.
# Viewing features
# %matplotlib inline
import matplotlib.pyplot as plt
#draw histogram of each feature
df.hist(bins=50, figsize=(20,15), color = "#A5737B")
#save_fig("attribute_histogram_plots")
plt.show()

# Correlation heatmap
plt.subplots(figsize=(12,5))
gender_correlation=df.corr()
sns.heatmap(gender_correlation,annot=True,cmap='RdPu')

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(20, 5))

(sns.scatterplot(data=df[df.y_ksi == 0], x="longitude", y="latitude", hue = 'y_ksi', alpha=0.05, palette=['#D0B4A6'],  ax=ax1)
  .set(title = 'Map of accidents : Not major injruy and killed', ylabel = 'Latitude', xlabel = 'Longitude'))

(sns.scatterplot(data=df[df.y_ksi == 1], x="longitude", y="latitude", hue = 'y_ksi', alpha=0.05, palette=["#A5737B"],  ax=ax2)
  .set(title = 'Map of accidents : Major injruy and killed', ylabel = 'Latitude', xlabel = 'Longitude'))

fig.show()

# Map
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(20, 5))

(sns.scatterplot(data=df[df.y_dth == 0], x="longitude", y="latitude", hue = 'y_dth', alpha=0.05, palette=['#D0B4A6'],  ax=ax1)
  .set(title = 'Map of accidents : Not killed', ylabel = 'Latitude', xlabel = 'Longitude'))

(sns.scatterplot(data=df[df.y_dth == 1], x="longitude", y="latitude", hue = 'y_dth', alpha=0.05, palette=['#A5737B'],  ax=ax2)
  .set(title = 'Map of accidents : Killed', ylabel = 'Latitude', xlabel = 'Longitude'))

fig.show()

# Days of the week
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(25, 5))
(sns.barplot(data=df, x="wkday_cat", y="y_ksi", palette="ch:.25", 
            order=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], ax=ax1)
  .set(title = 'KSI by weekday', ylabel = 'Poportion with major injuriy or killed', xlabel = 'Weekday'))

(sns.barplot(data=df, x="wkday_cat", y="y_dth", palette="ch:.25", 
            order=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], ax=ax2)
  .set(title = 'Fatality by weekday',  ylabel = 'Poportion killed', xlabel = 'Weekday'))
fig.show()

# Visibility 
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(25, 5))
sns.barplot(data=df, x="visibility", y="y_ksi", palette="ch:.25", ax=ax1).set(title = 'KSI by visibility')
sns.barplot(data=df, x="visibility", y="y_dth", palette="ch:.25", ax=ax2).set(title = 'Fatality by visibility')
fig.show()

# Light 
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(25, 5))
sns.barplot(data=df, x="light", y="y_ksi", palette="ch:.25", ax=ax1).set(title = 'KSI by light conditions')
sns.barplot(data=df, x="light", y="y_dth", palette="ch:.25", ax=ax2).set(title = 'Fatality by light conditions')
fig.show()

# Road conditions 
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(25, 5))
sns.barplot(data=df, x="rdsfcond", y="y_ksi", palette="ch:.25", ax=ax1).set(title = 'KSI by road conditions')
sns.barplot(data=df, x="rdsfcond", y="y_dth", palette="ch:.25", ax=ax2).set(title = 'Fatality by road conditions')
fig.show()

# Vehicle type 
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(25, 5))
sns.barplot(data=df, x="vehtype", y="y_ksi", palette="ch:.25", ax=ax1).set(title = 'KSI by vehicle type')
sns.barplot(data=df, x="vehtype", y="y_dth", palette="ch:.25", ax=ax2).set(title = 'Fatality by vehicle type')
fig.show()
#df['vehtype'].value_counts()

"""## Grouped dataset"""

# Creating binary crash level fatality variable
outcome = df.loc[df['injury'] == "Fatal"]
outcome = outcome[['accnum', 'year', 'wkday_cat', 'injury', 'y_dth']]
outcome = outcome.groupby(['accnum', 'year', 'wkday_cat'], as_index = False).first()

# Grouping by accnum, year and weekday
dfg = df.groupby(['accnum', 'year', 'wkday_cat'], as_index = False).first()

# Adding binary crash-level fatility variable
dfg['y'] = dfg['accnum'].isin(outcome['accnum'])
dfg['y'] = np.where((dfg['y'] == True), 1, 0)

# Adding hour variable
dfg['hour'] =  df['time'].astype(str).str[:-2]
dfg['hour'] =  np.where((dfg['hour'] == ''), '0', dfg['hour'])


# Fixing holiday to binary 
dfg['public_holiday'] = np.where((dfg['public_holiday'] == True), 1, 0)


# Filter year 
dfg = dfg.query("year >= 2014")


# Removing un-needed columns for ML
dfg = dfg.drop(columns=[ 'acclass', 'wardnum', 'street1', 'street2', 'hood_140', 'neighbourhood_140', 'division', 'neighbourhood_158', 'district',
    'year', 'date', 'injury','accloc', 'invtype', 'invage', 'time',
    'injury', 'fatal_no', 'initdir', 'vehtype', 'manoeuver', 'drivact',
    'drivcond', 'pedtype', 'pedact', 'pedcond', 'wkday', 'y_ksi', 
    'y_dth', 'accnum', 'offset', 'longitude', 'latitude']) 

dfg.head()

dfg.columns

# Check outcome balance
check_data_balance(dfg["y"]) # We have an unbalanced dataset

dfg.describe()

dfg.describe(include = object)

dfg.dtypes

dfg.isna().sum().sort_values()

"""### Data vis"""

# Days of the week
(sns.barplot(data=df, x="wkday_cat", y="y_dth", palette="ch:.25", 
            order=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])
  .set(title = 'Fatality by weekday',  ylabel = 'Poportion killed', xlabel = 'Weekday'))
fig.show()

# Visibility 
sns.barplot(data=dfg, x="visibility", y="y", palette="ch:.25").set(title = 'Fatality by visibility')
fig.show()

# Road conditions 
sns.barplot(data=dfg, x="rdsfcond", y="y", palette="ch:.25").set(title = 'Fatality by road conditions')
fig.show()

# Light
sns.barplot(data=dfg, x="light", y="y", palette="ch:.25").set(title = 'Fatality by light conditions')
fig.show()

# Alcohol
sns.barplot(data=dfg, x="alcohol", y="y", palette="ch:.25").set(title = 'Fatality by alcohol status')
fig.show()

# Alcohol
sns.barplot(data=dfg, x="month", y="y", palette="ch:.25").set(title = 'Fatality by month')
fig.show()

# Hour
sns.barplot(data=dfg, x="hour", y="y", palette="ch:.25").set(title = 'Fatality by hour')
fig.show()

dfg.y

"""# Machine Learning"""

# We can move these to appropriate chunks after
from sklearn import datasets, linear_model
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import KNNImputer, SimpleImputer
from sklearn.compose import make_column_transformer, make_column_selector
from sklearn.pipeline import make_pipeline, Pipeline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_validate
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import StratifiedKFold
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier
from mlxtend.plotting import plot_learning_curves

# Train test split for KSI y variable
from sklearn.model_selection import train_test_split

X = dfg.drop(columns=['y'])
# X = X[["hour", "road_class" , "traffctl", "visibility", "light", "rdsfcond"]]
y = dfg['y']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

X_train.head()

X_train.columns

"""## Main Pipeline

# Model Training
"""

# Setting threshold for evaluation metrics
threshold = 0.25 # Set to False if wanting to use no threshold

"""## Logistic"""

# Column types 
numeric_features = []#['wardnum'] #, 'latitude', 'longitude']
categorical_features = [item for item in X_train.columns.values if item not in numeric_features]

# Build a numeric pipeline
numeric_transformer = make_pipeline(   
    KNNImputer(),
    StandardScaler())

# Build a categorical pipeline
categorical_transformer = make_pipeline(
    SimpleImputer(strategy="most_frequent"),
    OneHotEncoder(handle_unknown="ignore"))

# Build a column transformer
col_transformer = make_column_transformer(
    (numeric_transformer, numeric_features),
    (categorical_transformer, categorical_features)
)


# Feature selction
from sklearn.feature_selection import SelectPercentile, chi2
selection = SelectPercentile(chi2, percentile = 50)

# Main pipeline
main_pipe = Pipeline(
            steps=[("ct", col_transformer),
                   ('sel', selection),
                   ("log", LogisticRegression())]
)

# Build a grid of the parameters you wish to search. 
param_grid = {
    "log__penalty" : ["none", "l1", "l2", "elasticnet"],
    "log__C" : [100, 10, 1.0, 0.1, 0.01]
}

# Conduct grid search with 10 fold cross-validation
log = GridSearchCV(main_pipe, param_grid, cv=10, verbose=1, n_jobs=-1)

# Fit your pipeline with grid search 
log.fit(X_train, y_train)

# Save the best hyperparameter values in an object named best_hyperparams
best_hyperparams = log.best_params_

# Print best_hyperparams
print(best_hyperparams)

# Save your results in an object named `train_score`
train_score = log.score(X_train, y_train)

# Display your score 
train_score

# Best model
log_best = log.best_estimator_
log_best.score(X_train, y_train), log_best.score(X_test, y_test)

# Determine threshold to use
# threshold = False # Set to False if wanting to use no threshold

if type(threshold)==bool :
  y_pred_train = log.predict_proba(X_train)[:,1]
  y_pred_test =  log.predict_proba(X_test)[:,1]
else : 
  y_pred_train = (log.predict_proba(X_train)[:,1] >= threshold).astype(int)
  y_pred_test =  (log.predict_proba(X_test)[:,1] >= threshold).astype(int)

# Creating AUC
from sklearn.metrics import roc_auc_score

auc_train = roc_auc_score(y_train, y_pred_train) 
auc_test = roc_auc_score(y_test, y_pred_test)

auc_train, auc_test

# AUC curve
from sklearn.metrics import roc_curve

# https://www.statology.org/plot-multiple-roc-curves-python/

fpr, tpr, _ = roc_curve(y_test, y_pred_test)
auc = round(roc_auc_score(y_test, y_pred_test), 4)
plt.plot(fpr, tpr, label="Logistic, AUC="+str(auc))
plt.title("AUC score for ML models predicting fatality")
plt.legend(loc='lower right')

# Confusion matrix
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

if type(threshold)==bool :
  predictions = log.predict(X_test)
else : 
  predictions = (log.predict_proba(X_test)[:,1] >= 0.25).astype(int)

cm = confusion_matrix(y_test, predictions, labels=log.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=log.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.show()

# Printing metrics
from sklearn.metrics import precision_score
precision = precision_score(y_test, predictions, labels=[1,2], average='micro')

from sklearn.metrics import recall_score
recall = recall_score(y_test, predictions, labels=[1,2], average='micro')

from sklearn.metrics import f1_score
f1 = f1_score(y_test, predictions, labels=[1,2], average='micro')


print("Logistic Regression ------------------------------------------")
print('Recall: ', round(recall, 3), ', Percision: ', round(precision, 3), ', F1: ', round(f1, 3)) 
print("\n\n")

# Calculate classification report
from sklearn.metrics import classification_report
print("Logistic Regression ------------------------------------------")
print(classification_report(y_test, predictions,
                            target_names=["Non-Fatal", "Fatal"]))

"""## Learning Curve for Logistic Regression"""

# Learning curve
plot_learning_curves(X_train, y_train, X_test, y_test, log)
plt.show()

"""## KNN"""

# Column types 
numeric_features = []#['wardnum']#, 'latitude', 'longitude']
categorical_features = [item for item in X_train.columns.values if item not in numeric_features]


# Build a numeric pipeline
numeric_transformer = make_pipeline(
    SimpleImputer(strategy="median"),
    StandardScaler())

# Build a categorical pipeline
categorical_transformer = make_pipeline(
    SimpleImputer(strategy="most_frequent"),
    OneHotEncoder(handle_unknown="ignore"))

# Build a column transformer
col_transformer = make_column_transformer(
    (numeric_transformer, numeric_features),
    (categorical_transformer, categorical_features))

main_pipe = Pipeline(
            steps=[("ct", col_transformer),
                   ("knn", KNeighborsClassifier())])


# Build a grid of the parameters you wish to search. 
param_grid = {
    "knn__n_neighbors" : [3, 5, 7, 9, 11, 13, 15, 17, 19],
    "knn__weights" : ['uniform', 'distance']
}

# Conduct grid search with 10 fold cross-validation
knn = GridSearchCV(main_pipe, param_grid, cv=10, verbose=1, n_jobs=-1)

# Fit your pipeline with grid search 
knn.fit(X_train, y_train)

# Save the best hyperparameter values in an object named best_hyperparams
best_hyperparams = knn.best_params_

# Print best_hyperparams
print(best_hyperparams)

# Save your results 
train_score = knn.score(X_train, y_train)

# Display your score 
train_score # I dont think we can use this since the dataset is unbalanced

# Determine threshold to use
# threshold = 0.25 # Set to False if wanting to use no threshold

if type(threshold)==bool :
  y_pred_train = knn.predict_proba(X_train)[:,1]
  y_pred_test =  knn.predict_proba(X_test)[:,1]
else : 
  y_pred_train = (knn.predict_proba(X_train)[:,1] >= threshold).astype(int)
  y_pred_test =  (knn.predict_proba(X_test)[:,1] >= threshold).astype(int)

# Creating AUC score
from sklearn.metrics import roc_auc_score

auc_train = roc_auc_score(y_train, y_pred_train) 
auc_test = roc_auc_score(y_test, y_pred_test)

auc_train, auc_test

# AUC curve
from sklearn.metrics import roc_curve

# https://www.statology.org/plot-multiple-roc-curves-python/

fpr, tpr, _ = roc_curve(y_test, y_pred_test)
auc = round(roc_auc_score(y_test, y_pred_test), 4)
plt.plot(fpr, tpr, label="KNN, AUC="+str(auc))
plt.title("AUC score for ML models predicting fatality")
plt.legend(loc='lower right')

# Confusion matrix
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

if type(threshold)==bool :
  predictions = knn.predict(X_test)
else : 
  predictions = (knn.predict_proba(X_test)[:,1] >= 0.25).astype(int)

cm = confusion_matrix(y_test, predictions, labels=knn.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.show()

# Printing metrics
from sklearn.metrics import precision_score
precision = precision_score(y_test, predictions, labels=[1,2], average='micro')

from sklearn.metrics import recall_score
recall = recall_score(y_test, predictions, labels=[1,2], average='micro')

from sklearn.metrics import f1_score
f1 = f1_score(y_test, predictions, labels=[1,2], average='micro')


print("K-Nearest Neighbours ------------------------------------------")
print('Recall: ', round(recall, 3), ', Percision: ', round(precision, 3), ', F1: ', round(f1, 3)) 
print("\n\n")

# Calculate classification report
from sklearn.metrics import classification_report
print("K-Nearest Neighbours ------------------------------------------")
print(classification_report(y_test, predictions,
                            target_names=["Non-Fatal", "Fatal"]))

"""## Learning Curve for N-Nearest Neighbours"""

# Learning curve
plot_learning_curves(X_train, y_train, X_test, y_test, knn)
plt.show()

"""## Decision Tree"""

# Column types 
numeric_features = []#['wardnum']#, 'latitude', 'longitude']
categorical_features = [item for item in X_train.columns.values if item not in numeric_features]

# Build a numeric pipeline
numeric_transformer = make_pipeline(
    #SimpleImputer(strategy="median"),
    KNNImputer(),
    StandardScaler())

# Build a categorical pipeline
categorical_transformer = make_pipeline(
    SimpleImputer(strategy="most_frequent"),
    OneHotEncoder(handle_unknown="ignore"))

# Build a column transformer
col_transformer = make_column_transformer(
    (numeric_transformer, numeric_features),
    (categorical_transformer, categorical_features)
)


# Pipeline
main_pipe = make_pipeline(col_transformer, 
                          DecisionTreeClassifier())

# Build a grid of the parameters you wish to search. 
param_grid = {
    "columntransformer__pipeline-1__knnimputer__n_neighbors" : [3, 5, 7, 9],
    "decisiontreeclassifier__criterion" : ['gini', 'entropy'],
    "decisiontreeclassifier__max_depth" : [2,4,6,8,10,12]
}


# Conduct grid search with 10 fold cross-validation
tree = GridSearchCV(main_pipe, param_grid, cv=10, verbose=1, n_jobs=-1)

# Fit your pipeline with grid search 
tree.fit(X_train, y_train)

# Save the best hyperparameter values in an object named best_hyperparams
best_hyperparams = tree.best_params_

# Print best_hyperparams
print(best_hyperparams)

# Save your results in an object named `train_score`
train_score = tree.score(X_train, y_train)

# Display your score 
train_score

# Best model
tree_best = tree.best_estimator_
tree_best.score(X_train, y_train)

# Determine threshold to use
# threshold = 0.25 # Set to False if wanting to use no threshold

if type(threshold)==bool :
  y_pred_train = tree.predict_proba(X_train)[:,1]
  y_pred_test =  tree.predict_proba(X_test)[:,1]
else : 
  y_pred_train = (tree.predict_proba(X_train)[:,1] >= threshold).astype(int)
  y_pred_test =  (tree.predict_proba(X_test)[:,1] >= threshold).astype(int)

# AUC score
from sklearn.metrics import roc_auc_score

auc_train = roc_auc_score(y_train, y_pred_train) 
auc_test = roc_auc_score(y_test, y_pred_test)

auc_train, auc_test

# AUC curve
from sklearn.metrics import roc_curve

# https://www.statology.org/plot-multiple-roc-curves-python/

fpr, tpr, _ = roc_curve(y_test, y_pred_test)
auc = round(roc_auc_score(y_test, y_pred_test), 4)
plt.plot(fpr, tpr, label="Decision Tree, AUC="+str(auc))
plt.title("AUC score for ML models predicting fatality")
plt.legend(loc='lower right')

# Confusion matrix
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

if type(threshold)==bool :
  predictions = tree.predict(X_test)
else : 
  predictions = (tree.predict_proba(X_test)[:,1] >= 0.25).astype(int)

cm = confusion_matrix(y_test, predictions, labels=tree.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=tree.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.show()

# Printing metrics
from sklearn.metrics import precision_score
precision = precision_score(y_test, predictions, labels=[1,2], average='micro')

from sklearn.metrics import recall_score
recall = recall_score(y_test, predictions, labels=[1,2], average='micro')

from sklearn.metrics import f1_score
f1 = f1_score(y_test, predictions, labels=[1,2], average='micro')


print("Decision Tree ------------------------------------------")
print('Recall: ', round(recall, 3), ', Percision: ', round(precision, 3), ', F1: ', round(f1, 3)) 
print("\n\n")

# Calculate classification report
from sklearn.metrics import classification_report
print("Decision Tree ------------------------------------------")
print(classification_report(y_test, predictions,
                            target_names=["Non-Fatal", "Fatal"]))

"""## Learning Curve for Decision Tree"""

# Learning curves
plot_learning_curves(X_train, y_train, X_test, y_test, tree)
plt.show()

"""## Random Forest"""

# Column types 
numeric_features = []#['wardnum']#, 'latitude', 'longitude']
categorical_features = [item for item in X_train.columns.values if item not in numeric_features]


# Build a numeric pipeline
numeric_transformer = make_pipeline(
    KNNImputer(),
    StandardScaler()
)

# Build a categorical pipeline
categorical_transformer = make_pipeline(
    SimpleImputer(strategy="most_frequent"),
    OneHotEncoder(handle_unknown="ignore"))

# Build a column transformer
col_transformer = make_column_transformer(
    (numeric_transformer, numeric_features),
    (categorical_transformer, categorical_features)
)

# Build a main pipeline
main_pipe = make_pipeline(col_transformer, 
                          RandomForestClassifier()
)

param_grid = {"randomforestclassifier__n_estimators" : [90, 100, 115, 130], 
              'randomforestclassifier__criterion': ['gini', 'entropy'], 
              "randomforestclassifier__max_depth" : range(2, 20),
              "randomforestclassifier__min_samples_leaf" : range(1, 10),
              "randomforestclassifier__min_samples_split" : range(2, 10),
              "randomforestclassifier__max_features" : ['auto', 'log2']
}

# Conduct grid search with 10 fold cross-validation
# grid_search = GridSearchCV(main_pipe, param_grid, cv=10, verbose=1, n_jobs=-1)
rf = RandomizedSearchCV(main_pipe, param_grid, cv=10, verbose=1, 
                        n_iter = 20, n_jobs=-1)

# Fit your pipeline with grid search 
rf.fit(X_train, y_train)

# Save the best hyperparameter values in an object named best_hyperparams
best_hyperparams = rf.best_params_

# Print best_hyperparams
print(best_hyperparams)

# Save your results in an object named `train_score`
train_score = rf.score(X_train, y_train)

# Display your score 
train_score

# Best model
rf_best = rf.best_estimator_
rf_best.score(X_train, y_train)

# Determine threshold to use
# threshold = 0.25 # Set to False if wanting to use no threshold

if type(threshold)==bool :
  y_pred_train = rf.predict_proba(X_train)[:,1]
  y_pred_test =  rf.predict_proba(X_test)[:,1]
else : 
  y_pred_train = (rf.predict_proba(X_train)[:,1] >= threshold).astype(int)
  y_pred_test =  (rf.predict_proba(X_test)[:,1] >= threshold).astype(int)

# AUC score
from sklearn.metrics import roc_auc_score

auc_train = roc_auc_score(y_train, y_pred_train) 
auc_test = roc_auc_score(y_test, y_pred_test)

auc_train, auc_test

# AUC curve
from sklearn.metrics import roc_curve

# https://www.statology.org/plot-multiple-roc-curves-python/

fpr, tpr, _ = roc_curve(y_test, y_pred_test)
auc = round(roc_auc_score(y_test, y_pred_test), 4)
plt.plot(fpr, tpr, label="Random Forest, AUC="+str(auc))
plt.title("AUC score for ML models predicting fatality")
plt.legend(loc='lower right')

# Confusion matrix
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

if type(threshold)==bool :
  predictions = rf.predict(X_test)
else : 
  predictions = (rf.predict_proba(X_test)[:,1] >= 0.25).astype(int)

cm = confusion_matrix(y_test, predictions, labels=rf.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.show()

# Printing metrics
from sklearn.metrics import precision_score
precision = precision_score(y_test, predictions, labels=[1,2], average='micro')

from sklearn.metrics import recall_score
recall = recall_score(y_test, predictions, labels=[1,2], average='micro')

from sklearn.metrics import f1_score
f1 = f1_score(y_test, predictions, labels=[1,2], average='micro')


print("Random Forest ------------------------------------------")
print('Recall: ', round(recall, 3), ', Percision: ', round(precision, 3), ', F1: ', round(f1, 3)) 
print("\n\n")

# Calculate classification report
from sklearn.metrics import classification_report
print("Random Forest ------------------------------------------")
print(classification_report(y_test, predictions,
                            target_names=["Non-Fatal", "Fatal"]))

"""## Learning Curves for Random Forest

"""

# Learning cures
plot_learning_curves(X_train, y_train, X_test, y_test, rf)
plt.show()

"""## Voting Classifier

"""

# Creating Voting Ensemble
voting_clf = VotingClassifier(
               estimators=[('logistic', log), ('KNN', knn), ('dtree', tree), ('randf', rf)]
              ,voting='soft'
              )

voting_clf.fit(X_train, y_train)

# Metrics
for clf in (log, knn, tree, rf):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print("train:",clf.__class__.__name__, accuracy_score(y_train, clf.predict(X_train)))
    print("test:",clf.__class__.__name__, accuracy_score(y_test, y_pred))
    print("==================================================================")

# Save your results in an object named `voting_score`
voting_score = voting_clf.score(X_train, y_train)
voting_score

# Determine threshold to use
# threshold = 0.25 # Set to False if wanting to use no threshold

if type(threshold)==bool :
  y_pred_train = voting_clf.predict_proba(X_train)[:,1]
  y_pred_test =  voting_clf.predict_proba(X_test)[:,1]
else : 
  y_pred_train = (voting_clf.predict_proba(X_train)[:,1] >= threshold).astype(int)
  y_pred_test =  (voting_clf.predict_proba(X_test)[:,1] >= threshold).astype(int)

# AUC score
from sklearn.metrics import roc_auc_score

auc_train = roc_auc_score(y_train, y_pred_train) 
auc_test = roc_auc_score(y_test, y_pred_test)

auc_train, auc_test

# AUC curve
from sklearn.metrics import roc_curve

# https://www.statology.org/plot-multiple-roc-curves-python/

fpr, tpr, _ = roc_curve(y_test, y_pred_test)
auc = round(roc_auc_score(y_test, y_pred_test), 4)
plt.plot(fpr, tpr, label="Voting Classifier, AUC="+str(auc))
plt.title("AUC score for ML models predicting fatality")
plt.legend(loc='lower right')

# Confusion matrix
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

if type(threshold)==bool :
  predictions = voting_clf.predict(X_test)
else : 
  predictions = (voting_clf.predict_proba(X_test)[:,1] >= 0.25).astype(int)

cm = confusion_matrix(y_test, predictions, labels=voting_clf.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=voting_clf.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.show()